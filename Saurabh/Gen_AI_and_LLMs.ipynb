{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118a0c60-5c34-4db8-871b-da1f8efe1194",
   "metadata": {},
   "source": [
    "# Generative AI and Large Language Models\n",
    "\n",
    "In this chapter we will explore recipes that use the generative aspect of the transformer models to generate text. As we touched upon the same in the `Transformers and its applications` chapter, the generative aspect of the transformer models uses the decoder component of the tranformer network. The decoder component is responsible for generating text based on the provided context.\n",
    "\n",
    "With the advent of the GPT (General Purpose Transformers) family of large language models (LLMs), these have only grown in size and capability with each new version. Large language models like GPT-4 have been trained on large corpora of text and can match or beat the state-of-the-art on many NLP tasks. These LLMs have also build upon their generational capability and they can be instructed to generate text based on human prompting.\n",
    "\n",
    "We will use generative models based on the transformer architecture for our recipes and explore:\n",
    "\n",
    "* How to run a generative model or a LLM locally on our system\n",
    "* How to make the LLM follow instruction prompting to generate text\n",
    "* How to augment the LLM with custom data to answer questions based on it\n",
    "* How to use different types of LLMs to generate code, SQL etc.\n",
    "* How to use agents to perform custom actions to retrieve data and augment an LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68099d3-d6b4-4dae-a066-9ec5b3780df6",
   "metadata": {},
   "source": [
    "\n",
    "# <h1><center>Running an LLM locally</center></h1>\n",
    "\n",
    "In this recipe, we will learn how to load a LLM locally using the CPU or GPU and generate text from it after giving it a starting text as seed input. We recommend that you use a system with at least 16 GB of RAM, or a system with a GPU that has at least 8 GB of VRAM. These examples were created on a system with 8 GB of RAM and an nVidia RTX 2070 GPU with 8 GB of VRAM. These examples will work without a GPU as long as there is 16 GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8550ce-4428-4cb1-bdd9-e405019ed19b",
   "metadata": {},
   "source": [
    "## How to do it\n",
    "\n",
    "In this recipe, we will load the `Mistral-7B` model using the HuggingFace libraries. This model has a smaller size compared to other language models in its class, but can outperform them on several NLP tasks. The `Mistral-7B` model with 7 billion parameters can outperform the `Llama 2` model that has over 13 billion parameters. For more information on this, please refer to the Mistral model details on https://mistral.ai/news/announcing-mistral-7b/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfcba3d-40a6-4589-ac5e-0c677566488d",
   "metadata": {},
   "source": [
    "1. Do the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95587402-30a9-4c90-8de8-3270a001c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe71513-0a68-4a7c-a10f-42a645ea85a6",
   "metadata": {},
   "source": [
    "2. Load the Mistral model `mistralai/Mistral-7B-v0.1` and the respective tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62fa69-df7f-4c44-8f66-3fd13eaa1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ae8f1-afac-4881-bfdc-3ec6437bb5bf",
   "metadata": {},
   "source": [
    "3. Initialize a generation config. This generation config is passed to the model instructing it on how to generate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da87fc62-5d12-426d-a31a-98bdaf63775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9cc06-c1c2-4a11-b175-d2bcc60f683d",
   "metadata": {},
   "source": [
    "4. Initialize a seed sentence. In this case, we ask the LLM to tell us a step by step process to make an apple pie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db5e074-1192-4c22-8f68-023bae2e2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sentence = \"Step by step way on how to make an apple pie:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58fe42-945d-4e6e-893e-a1fcfb11e413",
   "metadata": {},
   "source": [
    "5. Tokenize the seed sentence and get the model to generate the tokens IDs as the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b44aa9-19e7-4e85-91d0-1fc400c2f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([seed_sentence], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**model_inputs, generation_config=generation_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6049933-6479-4c65-94f4-551eebbbace3",
   "metadata": {},
   "source": [
    "6. Decode the model token IDs into text and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0034bd3-96bf-4786-8368-f174217323e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step by step way on how to make an apple pie:\n",
      "\n",
      "1. Preheat the oven to 200°C/400°F/gas mark 6.\n",
      "2. Roll out the pastry on a lightly floured surface to the thickness of a £1 coin.\n",
      "3. Line a 23cm/9in fluted tart tin with the pastry, leaving the excess pastry hanging over the edge.\n",
      "4. Prick the base of the pastry with a fork.\n",
      "5. Line the pastry with baking parchment and fill with baking beans.\n",
      "6. Bake for 15 minutes.\n",
      "7. Remove the baking beans and baking parchment.\n",
      "8. Bake for a further 10 minutes, or until the pastry is golden-brown.\n",
      "9. Remove from the oven and leave to cool.\n",
      "10. Reduce the oven temperature to 180°C/350°F/gas mark 4.\n",
      "11. Peel, core and slice the apples.\n",
      "12. Place the apples in a large bowl.\n",
      "13. Add the sugar, lemon juice and cinnamon.\n",
      "14. Mix well.\n",
      "15. Spoon the apple mixture into the pastry case.\n",
      "16. Roll out the remaining pastry on a lightly floured surface to the thickness of a £1 coin.\n",
      "17. Cut the pastry into 2.5cm/1in strips.\n",
      "18. Weave the pastry strips over the apples.\n",
      "19. Brush the pastry with the beaten egg.\n",
      "20. Bake in the oven for 30-35 minutes, or until the pastry is golden-brown.\n",
      "21. Remove from the oven and leave to cool for 10 minutes.\n",
      "22. Cut into slices and serve.\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "- 375g/13oz ready-made shortcrust pastry\n",
      "- 500g/1lb 2oz Bramley apples, peeled, cored and sliced\n",
      "- 100g/3½oz caster sugar\n",
      "- 1 tbsp lemon juice\n",
      "- ½ tsp ground cinnamon\n",
      "- 1 free-range egg, beaten, to glaze\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d4f37-bbed-4545-ab84-f4204dc45d26",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we initialize the device, the `mistralai/Mistral-7B-v0.1` model and tokenizer respectively. We set the *device_map* parameter to *auto*, which lets the pipeline pick the available device to use. We use the *load_in_4bit* parameter to *True*. This lets us load the quantized model for the inference (or generation) step. Using a quantized model consumes less memory and lets us load the model locally on systems with limited memory. The loading of quantized model is handled by the *AutoModelForCausalLM* module and it downloads a model from the HuggingFace hub that has been quantized to the bit-size specified in the parameter.\n",
    "\n",
    "In step 3, we initialize the text-generation configuration. We use the *num_beams* parameter to be 4. This parameter results in the generated text being more coherent and grammatically correct as the number of beams are increased. However, more number of beams also result in decoding (or text-generation) time. We set the *early_stopping* parameter to *True* as the generation of the next word is concluded as soon as the number of beams reaches the value specified in the *num_beams* parameter. The *eos_token_id* and *pad_token_id* are defaulted to use the models token IDs. These token IDs are used to specify the end-of-sentence and padding tokens that will be used by the model. The *max_new_tokens* parameter specifies the maximum number of tokens that will be generated. There are more parameters that can be specified for generating the text and we encourage the readers to play around with different values of the above specified parameters as well as any additional parameter for customizing the text generation. For more information, please refer to the transformer documentation on the GenerationConfig class at *https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py*.\n",
    "\n",
    "In step 4, we initialize a seed sentence. This sentence acts as a prompt to the model for it to generate a step-by-step way to make an apple pie. As we saw in the output above, the model did generate a step-by-step recipe with proper numbering and arrangement of the text for the ingredients too.\n",
    "\n",
    "In step 5, we tokenized the seed sentence to transform the text into the corresponding embedded representation and pass it to the model to generate the text. We also pass the *generation_config* instance to it. The model generates the token IDs as part of its generation.\n",
    "\n",
    "In step 6, we decode the token IDs that were generated from the previous step. We set the value of the *skip_special_tokens* to *True*. The transformer model uses special tokens like *CLS* or *MASK* and to generate the text as part of the training. We don't want these to be in the output as we want to generate pure text as part of our output. We print the decoded (or generated) text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00ef98-3ebc-4a07-9f67-f7fb061b90e1",
   "metadata": {},
   "source": [
    "# <h1><center>Running an LLM to follow instructions</center></h1>\n",
    "\n",
    "In this recipe, we will learn how to get an LLM to follow instructions via prompting. We will use the `mistralai/Mistral-7B-Instruct-v0.1` model for this recipe. This model is build on top of the `mistralai/Mistral-7B-v0.1` LLM and has been tuned to follow instructions via prompts. Let's get started.\n",
    "\n",
    "## How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655cd405-76c6-4a18-9cdc-77870b35d602",
   "metadata": {},
   "source": [
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59cfbb-f324-458f-82fb-3cae1460f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, logging\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b6837-4dc8-42fc-88ef-25939ff4a78e",
   "metadata": {},
   "source": [
    "2. Set the logging level and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53d018d-93b2-4d1d-a2c9-405de13e6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2eefa-3812-46a4-8bc8-010c2a18c912",
   "metadata": {},
   "source": [
    "3. Load the Mistral model `mistralai/Mistral-7B-Instruct-v0.1` and the corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34627d66-6a21-48b6-a9f0-78cde51dfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", \n",
    "                                             load_in_4bit=True, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb5b9f-0909-4ad6-8828-0d58e4d7feca",
   "metadata": {},
   "source": [
    "4. Create a prompt that sets up an instruction context that can be passed to the LLM. The LLM acts as per the instructions setup in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1c9ed6-93ab-46eb-9b0d-9045daeac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite country?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I am quite fascinated with Peru.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What can you tell me about Peru?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c31e8-3c6b-477d-ae99-bfdef4a34fb3",
   "metadata": {},
   "source": [
    "5. Set the generation config for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14478d3a-ab5f-4721-92ce-3cabf5de9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523633d-daad-4ce1-8eb7-e0ffab08086c",
   "metadata": {},
   "source": [
    "6. Tokenize the prompt load it into the device as setup in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb46d92-5bd1-48aa-b596-dbed444539c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\")\n",
    "model_inputs = encoded_prompt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf1b89-3ab9-4cfb-b792-82f65d106386",
   "metadata": {},
   "source": [
    "7. Call the *generate* method on the model to generate the output based on the tokenized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc5d21f-23c1-4dd9-9e92-07db27122193",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True,\n",
    "                               generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c0aad-52e1-4fdd-8164-c599aea3a5f8",
   "metadata": {},
   "source": [
    "8. Decode the output token IDs into text and print it. We can see that the LLM generated the content about Peru in a well articulated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6272d9a-4b18-4622-9624-2a3e57220d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite country? [/INST]Well, I am quite fascinated with Peru.</s>  [INST] What can you tell me about Peru? [/INST] Peru is a country located in South America, and it is the 20th largest country in the world by land area. It is bordered by Ecuador to the north, Colombia to the northeast, Brazil to the east, Bolivia and Chile to the south, and the Pacific Ocean to the west.\n",
      "\n",
      "Peru is known for its diverse geography and culture, with a range of landscapes including the Andes Mountains, the Amazon Rainforest, and the Pacific coastline. The country is also home to a number of ancient civilizations, including the Inca Empire, which was once the largest empire in pre-Columbian America.\n",
      "\n",
      "Peru is also known for its rich biodiversity, with a wide variety of flora and fauna found throughout the country. The Amazon Rainforest, in particular, is home to a vast array of plant and animal species, many of which are found nowhere else on Earth.\n",
      "\n",
      "In terms of its economy, Peru is one of the fastest-growing countries in South America, with a strong focus on mining, agriculture, and tourism. The country is also home to a number of world-class archaeological sites, including Machu Picchu, which is considered one of the Seven Wonders of the Ancient World.\n",
      "\n",
      "Overall, Peru is a fascinating country with a rich history, diverse culture, and stunning natural beauty.</s>\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7cb1a-7a55-4738-b379-bdb7ef7cd880",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we set the logging setting to error and lower its verbosity to keep the example output concise. We also initialize the device which in our case is *cuda*. \n",
    "\n",
    "In step 3, we initialize the `mistralai/Mistral-7B-Instruct-v0.1` model and the corresponding tokenizer. This model is based on the model `mistralai/Mistral-7B-v0.1` that we used in the previous recipe. This model has been fine-tuned for QA and interactive conversion. \n",
    "\n",
    "In step 4, we setup the prompt based on the model chat template. The prompt sets up an instruction for the model to follow. This chat template is used by the tokenizer in the subsequent step.\n",
    "\n",
    "In step 5, we initialize the generation configuration similar to the previous recipe.\n",
    "\n",
    "In step 6, we call the *apply_chat_template* method of the tokenizer with the prompt. This method reads the tokenizer's chat_template to ascertain the control tokens and format to use to tokenize the input accordingly. We load the tokenized input into the device.\n",
    "\n",
    "In step 7, we generate the output for the given tokenized chat template input.\n",
    "\n",
    "In step 8, we decode the token IDs returned by the model and print the result. As we observe from the output, the printed output also printed out the instruction that was sent to the model. The instruction follows the format of \n",
    "`<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]`. The `<s>` and `<\\s>` token specify the beginning and the end of the sentence respectively. The `[INST]` and `[/INST]` are the start and end of the instruction. In this particular case, we started our instruction with a conversation between the user and the agent. The conversation started with the question *What is your favourite country?*. This question was followed by the model answer in the form of *Well, I am quite fascinated with Peru.*. We then followed it up with another instruction by asking the question *What can you tell me about Peru?*. This methodology serves as a template for the LLM to learn our intent and generate an answer for the follow-up question based on the pattern we specified in our instruction prompt.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693e0a5-b219-410a-a24c-6229f81c09a2",
   "metadata": {},
   "source": [
    "# There's more\n",
    "\n",
    "Now that we have seen a way to instruct a model to generate text, we can just change the prompt and get the model to generate text for a completely different kind of question. Let's just change the prompt text to the following and use the same recipe to generate text based on the updated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e576c1ab-3b76-43f0-87fc-9b76d4613efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Mary is twice as old as Sarah presently. Sarah is 6 years old.?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, what can I help you with?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me in a step by step way on how old Mary will be after 5 years?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa4e5128-80a1-44be-b2f0-01d86747367f",
   "metadata": {},
   "source": [
    "<s> [INST] Mary is twice as old as Sarah presently. Sarah is 6 years old.? [/INST]Well, what can I help you with?</s>  [INST] Can you tell me in a step by step way on how old Mary will be after 5 years? [/INST] Sure, here's a step-by-step calculation:\n",
    "\n",
    "1. Sarah is currently 6 years old.\n",
    "2. Mary is twice as old as Sarah, so Mary is currently 6 x 2 = 12 years old.\n",
    "3. After 5 years, Mary will be 12 + 5 = 17 years old.\n",
    "\n",
    "So, Mary will be 17 years old after 5 years.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cbddb-1841-49cd-9955-ae8f0175cfc5",
   "metadata": {},
   "source": [
    "As we can see from the above output, the instruct model is able to understand the instruction quite clearly and is able to reason well and answered the question correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ea91d-bb4a-41c9-8cdf-e369e469014f",
   "metadata": {},
   "source": [
    "# <h1><center>Augmenting an LLM with external data</center></h1>\n",
    "\n",
    "In the following recipres, we will learn how to get an LLM to answer questions on which it has not been trained on. These could include information that was created after the LLM was trained. The content on the world wide web keeps getting added on a daily basis. There is no one LLM can be trained on that context everyday. The Retriever Augmented Generation or RAG frameworks allow us to augment the LLM with additional content that can be sent as input to it for generating content for downstream tasks. As a basic introduction to RAG, we will augment an LLM with some content from a few web pages and ask some questions pertaining to the content contained in those pages. For this recipe, we will first load the LLM and ask it a few questions without providing it any context. We will then augment this LLM with additional context and ask the same questions. We will compare and contrast the answers, which will demonstrate the power of the LLM when coupled with augmented content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa69c3-67d6-489b-8afe-808858237615",
   "metadata": {},
   "source": [
    "# <h1><center>Execute a simple prompt-to-LLM chain</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c9d3e-e53c-472e-aa21-146821723a11",
   "metadata": {},
   "source": [
    "In this recipe we will use the Langchain *https://www.langchain.com/* framework to demonstrate Langchain framework with an example based on Langchain Expression Language (LCEL). We recommend that the readers visit the specified URL and install the python libraries by following the documentation on the site. In general, the command `pip install langchain` should be sufficient to get started. Let's start with a simple recipe based on the Langchain framework and extend it in the recipes that follow from there on.\n",
    "\n",
    "So far in the book, we loaded all the models as part of the recipes. For the Langchain based recipes, we will improvise a bit on the model loading by storing them as out-of-process. This enables us to decouple to recipe code from the model loading code. This also brings in the additional benefits of faster recipe execution since the model is always resident in memory while we execute the recipe.\n",
    "\n",
    "\n",
    "## How to do it\n",
    "\n",
    "For the upcoming recipes, we will follow the Ollama framework (https://github.com/jmorganca/ollama) to load the models out of process. Follow the steps below to install the Ollama artifacts and run the `llama2` model. The following steps are for the Linux and WSL2 environments. For other OSes, please refer to the documentation on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7617e6-117f-4c21-abbb-09d42afa843f",
   "metadata": {},
   "source": [
    "1. Execute the following command and install the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a505d66-655f-40c1-acde-995e328af6c0",
   "metadata": {},
   "source": [
    "`curl https://ollama.ai/install.sh | sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f47461-69f4-4abb-9b63-836889c074d0",
   "metadata": {},
   "source": [
    "2. Run the `llama2` model locally by executing the following command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290c2eb-46bb-42e3-a616-baa8f727c39b",
   "metadata": {},
   "source": [
    "`ollama run llama2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbde019-12df-43a1-804c-0023ecaf6c1c",
   "metadata": {},
   "source": [
    "3. Once the model is up and running, we can now start with the recipe. Start with doing the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471a5402-4745-400f-baa2-db4c8a823919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510260a-ae9e-43b4-8b35-e770444a6e70",
   "metadata": {},
   "source": [
    "4. Initialize the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570d9873-ebf6-42dd-bf57-20e122780de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama2\"\n",
    "llm = Ollama(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a66f32-25ce-4821-8b7f-08983bf946f5",
   "metadata": {},
   "source": [
    "5. We initialize a chat prompt template which is of the defined type `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e96768-8a8a-40b6-bcfc-b757759c9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a great mentor.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9394dc-0871-4729-b0b0-e8f80a7b171b",
   "metadata": {},
   "source": [
    "6. We initialize an output parser that is of the type `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5acd9f85-cbd6-4476-afb5-b2d4f6dac7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541221c-ec9a-489a-b027-f45cabc4c44d",
   "metadata": {},
   "source": [
    "7. We initialize an instance of a chain. This chain passes the prompt to the llm, and the output of the llm is passed to the output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cec462-5cd2-4dfc-b90f-32a7be50985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba851459-2db3-495d-867b-3c7f6cf91ca1",
   "metadata": {},
   "source": [
    "8. We create a simple query and ask the LLM to answer it. As we observe from the output, the advice presented by this example is actually a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be201c0-da34-4830-9278-91f055195fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a mentor, I'm glad you asked! Improving your software engineering skills is an ongoing process, and there are several ways to do so. Here are some suggestions:\n",
      "\n",
      "1. Practice, practice, practice: The more you work on actual projects, the better you'll become at software engineering. Look for opportunities to contribute to open-source projects or volunteer to work on small projects for friends or family.\n",
      "2. Learn new programming languages and technologies: Stay up-to-date with the latest tools and trends in the industry by learning new programming languages, frameworks, and tools.\n",
      "3. Focus on problem-solving skills: Good software engineers are able to break down complex problems into smaller, manageable parts and come up with creative solutions. Practice solving problems on platforms like HackerRank or LeetCode to improve your problem-solving skills.\n",
      "4. Learn about software design patterns: Understanding software design patterns can help you write more maintainable, scalable code. Study the SOLID principles and learn how to apply them in your code.\n",
      "5. Read books and articles: Stay current with the latest industry trends and best practices by reading books and articles on software engineering. Some recommended books include \"Clean Code\" by Robert C. Martin, \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas, and \"Software Architecture: Patterns, Principles, and Practices\" by Mark Richards.\n",
      "6. Participate in coding challenges: Join online communities like GitHub or CodeWars to participate in coding challenges and practice your skills in a collaborative environment.\n",
      "7. Take online courses or attend workshops: There are many online courses and workshops available that can help you improve your software engineering skills. Look for courses on platforms like Udemy, Coursera, or edX, or attend local workshops and conferences.\n",
      "8. Learn about testing and debugging: Good software engineers know how to test and debug their code effectively. Study testing methodologies and learn how to use debugging tools to identify and fix errors in your code.\n",
      "9. Learn about agile development methodologies: Many software engineering teams use agile methodologies like Scrum or Kanban to manage their projects. Learn about these methodologies and how they can help you work more efficiently.\n",
      "10. Network with other engineers: Connecting with other software engineers can help you learn new techniques, get feedback on your work, and stay motivated. Attend industry events, join online communities, or participate in hackathons to network with other engineers.\n",
      "\n",
      "Remember, improving your software engineering skills takes time and practice. Be patient, persistent, and always keep learning!\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"how can i improve my software engineering skills?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f2d07-b3d2-4676-b1ae-e42ac0fb7786",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we installed the Ollama framework. This framework allows us to host the model out-of-process and connect to the model using a client interface. The Ollama framework manages the connection to the model and communicating with it.\n",
    "\n",
    "In step 2, we run the Ollama runtime with the argument of `llama2`. The Ollama framework loads the `llama2` 7B model which has 7-billion parameters. The Ollama runtime allows loading bigger `llama2` models along with models from other providers. Once the command runs, it will start hosting the `llama2` model locally and is ready to accept commands.\n",
    "\n",
    "In step 3, we do the necessary imports.\n",
    "\n",
    "In step 4, we initialize the model. In our case we use the `llama2` model and wrap in the Ollama wrapper. The Ollama framework allows us to interface with the model using REST (or WebAPI) calls. The model is already running in a separate process as set up in step 2.\n",
    "\n",
    "In step 5, we initialize a chat prompt template instance using the `ChatPromptTemplate` class. The `from_messages` method takes a series of (message type, template) tuples. The second tuple in the messages array has the *{input}* template. This signifies that this value will be passed later.\n",
    "\n",
    "In step 6, we initialize an output parser. The `StrOutputParser` converts a chat message returned by an LLM instance to a string.\n",
    "\n",
    "In step 7, we initialize the chain. The chain pipes the output of one component to the next. In this instance, the *prompt* is sent to the **llm* and the llm operates on the prompt instance. The output of this operation is a chat message. The chat message is then sent to the output_parser, which converts it into a string. In this step, we only setup the various components of the chain. \n",
    "\n",
    "In step 8, we invoke the chain and print the results. We pass the input argument in a dictionary. We set up the prompt template as a message that had the *{input}* placeholder defined there. As part of the chain invocation, the input argument is passed through to the template. The chain invokes the command. The chain is instructing the LLM to generate the answer to the question it asked via the prompt that we setup previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed29fab-29d0-4ebe-a2a1-bf83886ee74d",
   "metadata": {},
   "source": [
    "# <h1><center>Augment the LLM with external content</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6552e1c-cf68-4da8-93f0-497922b4cc94",
   "metadata": {},
   "source": [
    "In this recipe, we will extend upon the previous example and build a chain that passes external content to the LLM and helps it answer questions based on that augmented content. We will first execute the chain without passing any external content and contrast the answer with the results when the chain is augmented with the external content. To keep the recipe simple, we will retrieve data from a recent web page that was published on the web after the model was trained. This ensures that the model is not answering from its memory and we will be able to better appreciate the role of content augmentation as part of this recipe. Let's get started.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330b4616-ea19-4f02-8711-19333cf080fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21613e08-f8c2-443c-b8c7-6f600135d78d",
   "metadata": {},
   "source": [
    "2. Initialize the `llama2` model and `Ollama` based embeddings provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c69179-7c3f-44a4-b94b-f6cd230eb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings_provider = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49574e3-4cc9-4c24-8a55-1001d6bec311",
   "metadata": {},
   "source": [
    "3. In this recipe we will load the wikipedia entry on the 2024 summer olympics. We initialize a WebBaseLoader object and pass it the wikipedia URL for the 2024 summer olympics. We call the *load* method for the loader instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81577d57-0728-40c2-8668-2d1e7bb397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\"https://en.wikipedia.org/wiki/2024_Summer_Olympics\"\n",
    "     ]\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee443a-5c2b-4417-a6fc-7b8d878a983b",
   "metadata": {},
   "source": [
    "4. Initialize the text splitter instance of the type `RecursiveCharacterTextSplitter`. Use the text splitter instance to split the documents into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac89676-9b26-43f0-b0d8-b81d2ef29ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed8f6c-17de-4b52-9e04-6ba62f5b5fff",
   "metadata": {},
   "source": [
    "5. We initialize a in-memory vector or embeddings store from the document chunks that we created in the previous step. We pass it the document chunks and the embeddings provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df94824-bd38-43c5-b57b-c8e134428a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = DocArrayInMemorySearch.from_documents(document_chunks, embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab9508-b47e-45b7-ab47-2f383a6e423b",
   "metadata": {},
   "source": [
    "6. Define a question-answer template and instantiate a `ChatPromptTemplate` instance from the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a2d630-faee-41bf-b35c-8f91f0af48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "                \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312cf507-e02c-4c23-90cd-44ff906535ce",
   "metadata": {},
   "source": [
    "7. Initialize the vector store retriever and the output parser. The retriever will provide the augmented content to the chain via the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96eca3ed-2290-4762-961b-b8dacba28299",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "                search_type=\"similarity\"\n",
    "            )\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f22f47-3a6f-41dc-bf33-e14827d4e771",
   "metadata": {},
   "source": [
    "8. We initialize an instance of `RunnableParallel` so that the arguments can be passed across the various components of the chain in an implicit manner. Then we initialize the chain that chains together the prompt, llm and the output parser along with the `RunnableParallel` instance in the beginning of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08969c2e-b89e-4c85-842d-3b58376d2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | llm | output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353b461-702e-4097-93fa-3acd8bd2ccc9",
   "metadata": {},
   "source": [
    "9. Invoke the chain and print the results. As we observe in this case, the answers returned by the chain are accurate, though I am skeptical whether BreakDancing is indeed a sport as returned in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a6fedc-f47c-4165-b173-37bc98697ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2024 Summer Olympics are being held in Paris, France.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"Where are the 2024 summer olympics being held?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364be244-9c25-4805-9eaf-cad1cf164555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The 2024 Summer Olympics are expected to feature several new sports and disciplines, including:\n",
      "\n",
      "1. Surfing: After making its debut at the 2016 Rio Olympics, surfing will return as a full medal sport in 2024.\n",
      "2. Skateboarding: Skateboarding was introduced as an Olympic sport for the first time in 2020, and it will continue to be featured at the 2024 Games.\n",
      "3. Rock climbing: Rock climbing made its Olympic debut in 2020, and it is expected to return in 2024.\n",
      "4. Breaking: Also known as breakdancing, breaking will make its Olympic debut in 2024 as a competitive dance sport.\n",
      "5. Karate: Karate was added to the Olympic program in 2018 and will be featured at the 2024 Games.\n",
      "6. Sport climbing: Sport climbing was introduced as an Olympic sport in 2020 and will continue to be featured in 2024.\n",
      "7. Urban mountain biking: This new discipline is a variation of mountain biking that takes place on urban terrain, such as parks and streets.\n",
      "8. 3x3 basketball: This version of basketball features a smaller court and fewer players than traditional basketball.\n",
      "9. Skateboarding park: This event will feature skateboarders competing in a park setting, with a focus on tricks and style.\n",
      "10. BMX freestyle: BMX freestyle will make its Olympic debut in 2024, with riders competing in a variety of events, including street and park competitions.\n",
      "\n",
      "These new sports and disciplines are expected to add excitement and diversity to the 2024 Summer Olympics, while also providing opportunities for athletes from around the world to compete at the highest level.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What are the new sports that are being added for the 2024 summer olympics?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd69a7d5-4d64-4226-b78b-c13499936bf2",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports. \n",
    "\n",
    "In step 2, we load the `llama2` model via the Ollama wrapper. The model make a call to the Ollama runtime in the later steps. We also load the embedding provider via the `OllamaEmbeddings` instance. The provider connects to the running instance of the Ollama runtime via REST. The default value for the embeddings model is `llama2`. It is a requirement that the Ollama runtime has loaded the `llama2` model and is running in the background when we make call to via the chain execution later. The model (or LLM) and the embeddings provider depend on it.\n",
    "\n",
    "In step 3, we load the URLs via the WebBaseLoader class. It extracts the HTML content and the main content on each HTML page is parsed. The *load* method on the loader instance triggers the extraction of the content from the URLs.\n",
    "\n",
    "In step 4, we initialize the text splitter instance and call the *split_documents* method on it. This splitting of the document is needed step as an LLM can only operate on a context of a limited length. For some large documents, the length of the document exceeds the maximum context length supported by the LLM. Breaking a document into chunks and using that to match the query text allows us to retrive more relevant parts from the document. The `RecursiveCharacterTextSplitter` splits the document based on newline, spaces, and double-newline characters.\n",
    "\n",
    "In step 5, we initialize a vector store. This vector store used in our recipe is an in-memory store. It is recommended that a vector database with persistency support is used in production. To keep this example self-contained and require fewer components, we decided to use an in-memory vector store. We initialize the vector store with the document chunks and the embeddings provider. The vector store creates embeddings of the documents chunks and stores them along with the document metadata. For production grade applications, we recommend the reader to visit the URL: https://python.langchain.com/docs/integrations/vectorstores/ and select a vector store based on their requirements. The Langchain framework is versatile and works with a host of prominent vector stores.\n",
    "\n",
    "In step 6, we define a chat template and create an instance of `ChatPromptTemplate` from it. This prompt template instructs the LLM to answer the question for the given context. This context is provided by the augmentation step.\n",
    "\n",
    "In step 7, we initialize a retriever and an output parser. For the retriever, we call the *as_retriever* method of the vector store instance. The retriever returned by the method is used to retrieve the content from the vector store. The *as_retriever* method is passed an argument `search_type` with the value *similarity*, which is also the default option.. This means that the vector store will be searched against the question text based on similarity. The other options supported are *mmr*, which penalizes search results of the same type and returns diverse results, and *similarity_score_threshold* which operates the same as the *similarity* search type, but can filter out the results based on a threshold. These option also support an accompanying dictionary argument that can be used to tweak the searching parameters. We recommend that the readers refer to the Langchain documentation and tweak the parameters based on their requirements and empirical findings.\n",
    "\n",
    "In step 8, we set up the chain. The chain sequence starts with the `setup_and_retrieval` component that sets up the retriever as context provider. The *question* argument is assumed to be passed later on by the chain. The next component is the `prompt`, which is supplied the context value and the populated prompt is sent to the `llm`. The llm pipes or forwards the results to the string output_parser, which is designed to return the string contained in the output of the llm. There is no execution happening in this step. We are only setting up the chain.\n",
    "\n",
    "In steps 9 and 10, we invoke the chain with two different questions. For each invocation, the question text is matched by similarity against the vector store, the relevant document chunks are returned, followed by the llm using these document chunks as context and using it to answer the respective questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba33b1-189b-472d-a362-f51feb48e062",
   "metadata": {},
   "source": [
    "# <h1><center>Create a Chatbot using an LLM</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c26f4-9e06-4023-9ecc-9bdec70b059e",
   "metadata": {},
   "source": [
    "In this recipe, we will create a chatbot using the Langchain framework. We learnt in the previous recipe on how to ask questions to an LLM based on a piece of content. Though the LLM was able to answer questions accurately, the interaction with the LLM was completely stateless. The LLM is looking at each question in isolation and ignores any previous interactions or questions that it was asked. In this recipe, we will use an LLM to create a chat interaction, where it will be aware of the previous conversations and use the context from them to answer subsequent questions.\n",
    "\n",
    "## How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734ec26-0db5-4dfb-95b4-b6170a509228",
   "metadata": {},
   "source": [
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c464f21f-d343-43df-9000-50358bd7e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "import bs4\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928d84b-6528-45d4-8d8a-193e95165257",
   "metadata": {},
   "source": [
    "2. Initialize the `llama2` model and `Ollama` based embeddings provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5449cf13-4dcf-493f-b3e1-c45dd4aecf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings_provider = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e987f-05fc-4ad1-a397-c493866a07a3",
   "metadata": {},
   "source": [
    "3. In this recipe we will load a web page that has the content based on which we would want to ask questions. You are open to choose any webpage of your choice. We initialize a WebBaseLoader object and pass it the URL. We call the *load* method for the loader instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ada4bd-8866-40a8-b06f-0e6a7f9bcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "     ]\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a1653-4ebe-4234-9bfd-fa906f3711e5",
   "metadata": {},
   "source": [
    "4. Initialize the text splitter instance of the type `RecursiveCharacterTextSplitter`. Use the text splitter instance to split the documents into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe65744-bc63-4894-b90f-119596cf3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ca970-2d18-478f-8f2e-61af4882f3d7",
   "metadata": {},
   "source": [
    "5. We initialize a in-memory vector or embeddings store from the document chunks that we created in the previous step. We pass it the document chunks and the embeddings provider. We also initialize the vector store retriever and the output parser. The retriever will provide the augmented content to the chain via the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9046dfb8-0d97-40d7-ae26-bed550a7ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = DocArrayInMemorySearch.from_documents(document_chunks, embeddings_provider)\n",
    "retriever = vector_store.as_retriever()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f2c68-a05b-4665-baf3-1ea772252cc0",
   "metadata": {},
   "source": [
    "6. We define a system prompt that instructs the system to formulate a question based on the previous chat context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab2a9d5-dbfc-4c12-b45a-188124b0a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775931c-4950-4e64-9a1c-e4c47f3f07cb",
   "metadata": {},
   "source": [
    "7. We initialize a contexualized chain that uses the contextual system prompt created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819b138f-de86-49d2-a11e-f81cac3e56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_chain = contextualize_q_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff90c79-688a-4b11-af65-44dc40dfa46e",
   "metadata": {},
   "source": [
    "8. We initialize another system prompt. This system prompt is setup to use chat history to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002f7390-ed80-4204-a983-e662eac7b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef05823-5584-485e-a853-806a971baf8e",
   "metadata": {},
   "source": [
    "9. We initialize two helper methods. The `contextualized_question` method returns the contextualized chain if a chat history exists, else it returns the input question. The `format_docs` method concatenates the page content for each document separated by two newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bfdd7a-2094-444e-9da0-a2135c53904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c5189-ad5d-4469-9147-a8df158c6404",
   "metadata": {},
   "source": [
    "10. We initialize a chain that uses the contextualized question as the input, pipes it to the retriever  ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2ed080-fe18-4920-8c6f-d28b91498deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=contextualized_question | retriever | format_docs\n",
    "        )\n",
    "        | qa_prompt\n",
    "        | llm\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53207b69-84e1-4a76-94e6-09cf5afa4fd6",
   "metadata": {},
   "source": [
    "11. We initialize a chat_history and invoke the chain to answer a question. We extend the chat history sequence with the returned answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4fae2c6-dca3-4081-bd58-c449ee0d658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The model can be used for a variety of applications, such as text generation, language translation, and conversational dialogue.\n",
      "\n",
      "The key characteristics of an LLM include:\n",
      "\n",
      "1. Large scale: LLMs are trained on vast amounts of text data, typically in the range of tens to hundreds of millions of words. This allows the model to learn patterns and relationships in language that would be difficult or impossible to capture with smaller models.\n",
      "2. Deep learning architecture: LLMs typically use deep learning architectures, such as transformer-based models, to process and generate text. These models are trained using backpropagation and other machine learning techniques to optimize the model's performance.\n",
      "3. Multi-task learning: Many LLMs are designed to perform multiple tasks simultaneously, such as language translation, text summarization, and language generation. This allows the model to learn a wide range of linguistic patterns and relationships.\n",
      "4. Flexibility and adaptability: LLMs are often designed to be flexible and adaptable, allowing them to generate text that is context-dependent and tailored to specific tasks or applications.\n",
      "5. Ability to generalize: LLMs are trained on large datasets of text, which allows them to learn patterns and relationships that can be applied to a wide range of linguistic contexts. This enables the model to generate text that is coherent and natural-sounding, even when the input context is unfamiliar or unexpected.\n",
      "6. Improved performance with larger datasets: As the size of the training dataset increases, so does the model's ability to generalize and perform well on unseen data. This makes LLMs increasingly effective as the size of the training dataset grows.\n",
      "7. Potential for creative applications: LLMs have the potential to be used in a wide range of creative applications, such as generating new forms of poetry or fiction, or even creating entirely new languages.\n",
      "8. Ability to learn from human feedback: Many LLMs are designed to incorporate human feedback and corrections, which allows them to learn and improve over time. This can help the model generate more coherent and natural-sounding text.\n",
      "9. Capability to handle complex language tasks: LLMs are capable of handling complex language tasks, such as understanding nuanced meanings and contextual relationships in text, and generating text that is sensitive to these subtleties.\n",
      "10. Continued advancements in technology: As machine learning and natural language processing technologies continue to evolve, so too will the capabilities of LLMs, enabling them to perform an even wider range of tasks and applications.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is a large language model?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "print(ai_msg)\n",
    "chat_history.extend([HumanMessage(content=question), AIMessage(content=ai_msg)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad6ac1-85db-455f-84a2-89c25bb3251a",
   "metadata": {},
   "source": [
    "12. We invoke the chain again with a second question that needs contextualizing. We provide the chain with the chat history and print the answer to the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838d1946-95a3-4b71-83e3-18e1f997d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! The term \"large\" in the context of large language models (LLMs) refers to the scale and scope of the training data used to train these models. Here are some reasons why LLMs are called \"large\":\n",
      "\n",
      "1. Volume of training data: LLMs are trained on vast amounts of text data, typically ranging from tens to hundreds of millions of words. This allows the model to learn patterns and relationships in language that would be difficult or impossible to capture with smaller models.\n",
      "2. Depth of training data: In addition to the sheer volume of data, LLMs are often trained on diverse and representative datasets that cover a wide range of linguistic styles, genres, and contexts. This helps ensure that the model can handle complex language tasks and generate coherent text in a variety of settings.\n",
      "3. Complexity of model architecture: LLMs typically use deep learning architectures with many layers and parameters, which allow them to learn complex patterns and relationships in language. These models are often more complex than smaller models, which makes them more effective at handling complex language tasks.\n",
      "4. Multi-task learning capabilities: Many LLMs are designed to perform multiple tasks simultaneously, such as language translation, text summarization, and language generation. This allows the model to learn a wide range of linguistic patterns and relationships, making it more flexible and adaptable in different contexts.\n",
      "5. Adaptability to new data: As new data becomes available, LLMs can be fine-tuned to adapt to these changes and improve their performance over time. This makes them increasingly effective as the size of the training dataset grows.\n",
      "6. Improved generalization capabilities: With larger datasets, LLMs can learn more abstract and generalizable patterns in language, which allows them to generate text that is more coherent and natural-sounding even when the input context is unfamiliar or unexpected.\n",
      "7. Creative potential: The sheer scale of LLMs gives them the potential to be used in a wide range of creative applications, such as generating new forms of poetry or fiction, or even creating entirely new languages.\n",
      "8. Ability to incorporate human feedback: Many LLMs are designed to incorporate human feedback and corrections, which allows them to learn and improve over time. This can help the model generate more coherent and natural-sounding text.\n",
      "9. Continued advancements in technology: As machine learning and natural language processing technologies continue to evolve, so too will the capabilities of LLMs, enabling them to perform an even wider range of tasks and applications.\n",
      "\n",
      "In summary, the term \"large\" in the context of LLMs refers to the scale and scope of the training data used to train these models, as well as their complexity and multi-task learning capabilities.\n"
     ]
    }
   ],
   "source": [
    "second_question = \"Can you explain the reasoning behind calling it large?\"\n",
    "second_answer = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n",
    "print(second_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbe1ec-3450-4d92-8e3b-c74372dc2764",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "For steps 1-5, we perform the same steps as we did in the recipe prior to this. To avoid repetition, we ask the reader to refer to the explanations in the *How it works* section of the previous recipe.\n",
    "\n",
    "In step 6, we initialize a contextualized system prompt. A *system* prompt defines the persona and the instruction that is to be followed by the LLM. In this case, we use the system prompt to contain the instruction that the LLM has to use the chat history to formulate a standalone question. We initialize the prompt instance with the system prompt definition and set it up with the expectation that it will have access to the *chat_history* variable that will be passed to it at run-time. We also set it up with the *question* template that will also be passed at run-tume. \n",
    "\n",
    "In step 7, we initilize the contextualized chain. This chain uses the chat history and a given follow-up question from the user and sets up the context for it. As you saw in step 11 and 12 previously, where we asked a simple question followed by another question. The subsequent question did not provide any context and just referred to it as `it`. \n",
    "\n",
    "In step 8, we initialize a system prompt similar to what we did in the previous recipe based on RAG. This prompt just sets up a prompt template. However, we pass this prompt a contextualized question as the chat history grows. This prompt always answers a contextualized question, barring the first one.\n",
    "\n",
    "In step 9, we define two helper functions that are passed to the RAG chain created downstream. The `contextualized_question` method returns the original question as posed by the user if the chat history is not present. This is the typical scenario for the first question. Once the chat_history is present, it returns the contextualized chain. The `format_docs` method just appends the content of all the repository docs separated by newlines.\n",
    "\n",
    "In step 10, we set up a chain. We use the `RunnablePassthrough` class to set up the context. The `RunnablePassthrough` class by definition allows us to pass the input or add additional data to the input via dictionary values. The `assign` method will take a key and will assign the value to this key. In this case, the key is `context` and the assigned value for it is the result of the chained evaluation of the *contextualized question*, the *retriever*, and the *format_docs*. Putting that in the perspective of the whole recipe, for the first question, the context will the set of matched records for the question. For the second question, the context will use the *contextualized question* from the chat history, retrieve a set of matching records and pass that as the context.\n",
    "\n",
    "In step 11, we initialize a chat history array. We ask a simple question to the chain by invoking it. What happens internally is that the question in this case is the contextualized question since there is no chat history present at this point. The rag_chain just answers the question simply and prints the answer. We also extend the chat_history with the returned message. \n",
    "\n",
    "In step 12, we ask the chain a subsequent question without providing much contextual cue. Internally, the `rag_chain` and the `contextualize_q_chain` work in tandem to answer this question. The `contextualize_q_chain` uses the chat history to add more context to the question, retrieves matched records, and sends that as context to the `rag_chain`. The `rag_chain` used the context and the contextualized question to answer the subsequent question.\n",
    "\n",
    "**Note**: We provided a basic workflow on how to execute RAG based flows. We recommend that the reader refer the langchain documentation and use the necessary components to run solutions in production. Some of these would be, using a persistent vector DB store, using concrete types like `BaseChatMessageHistory:` and `RunnableWithMessageHistory` to better manage chat histories. Also, use LangServe to expose endpoints to serve requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf50b2-18a9-45e0-8eb4-e9d2a285e0a2",
   "metadata": {},
   "source": [
    "# <h1><center>Generate code using an LLM</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b218aa8-4482-40dd-a096-bbbe90531de1",
   "metadata": {},
   "source": [
    "In this recipe we will explore how as LLM can be used to generate code, and we will validate it by executing the generated code.\n",
    "\n",
    "\n",
    "\n",
    "## How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3362a0-a795-4187-853b-b526e56ca41f",
   "metadata": {},
   "source": [
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22eb79a5-85f2-408d-9d45-5e6aff07671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce017f14-b14e-4bb8-8400-abcaa3429d45",
   "metadata": {},
   "source": [
    "2. Define a template and initialize a prompt template from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd5b369-82ae-4247-9ddb-7196a0c2c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed920bd2-56e5-4020-8e61-705407385c42",
   "metadata": {},
   "source": [
    "3. Initilize the model. In this instance, as we are working to generate code, we choose to use the `codellama` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac9ce09-29ba-4368-9d68-b1f81a266e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model=\"codellama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd173f74-24a4-4185-8431-d26e22210337",
   "metadata": {},
   "source": [
    "4. We initialize the chain with the prompt and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b10b001-971f-4eda-b897-6e69dd9ca3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a4a10-dd73-42f4-b287-c56f68807f10",
   "metadata": {},
   "source": [
    "5. We invoke the chain and print the result. As we can observe from the output, then code generated by the model looks fairly good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e202a491-0dec-4156-ac56-4d5cddf1da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```\n",
      "class Node:\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "        self.left = None\n",
      "        self.right = None\n",
      "\n",
      "def inorder_traversal(root):\n",
      "    if root is None:\n",
      "        return []\n",
      "    return inorder_traversal(root.left) + [root.value] + inorder_traversal(root.right)\n",
      "\n",
      "root = Node(1)\n",
      "root.left = Node(2)\n",
      "root.right = Node(3)\n",
      "print(inorder_traversal(root)) # Output: [2, 1, 3]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"write a program to print a binary tree in an inorder traversal\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edb3d1-5d9d-44a6-a201-6c1391fbbf28",
   "metadata": {},
   "source": [
    "6. Let's attempt to do something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c958e9c-27a0-4253-8ea3-a1a1f0cdc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```python\n",
      "import hashlib\n",
      "\n",
      "def generate_aes_hash(input_str):\n",
      "    \"\"\"Generate a 512-bit AES hash from a given input string\"\"\"\n",
      "    return hashlib.sha512(input_str.encode()).hexdigest()\n",
      "```\n",
      "This function takes a string as an input and returns a hexadecimal representation of the SHA-512 hash of that string using the `hashlib` module in Python. The resulting hash will be 512 bits long, which is the length of a SHA-512 hash.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"write a program to generate a 512-bit AES hash\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1eb5cd-122d-4e22-b5cf-ab030caeb64e",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we define a template. This template defines the instruction or the system prompt that is sent to the model as the task description. In this case, the template defines an instruction to generate python code based on user's requirement. We use this template to initialize a prompt object. The initilized object is of the type `ChatPromptTemplate`. This object lets us send requirements to the model in an interactive way. We can converse with the model based on our instructions to generate several code snippets without having to load the model each time.\n",
    "\n",
    "In step 3, we initialize the `codellama` model. The model loaded via the Ollama framework loads the trained `Llama 2` model. This model trains the `Llama2` model on code specific datasets and supports code generation in Python, C++, Java, C# and more.\n",
    "\n",
    "In step 4, initiliaze the chain with the prompt, model and then output parser.\n",
    "\n",
    "In step 5, we invoke the chain and print the generated code. The generated code is reasonably good, with the Node class having a constructor along the *inorder_traversal* helper method. It also prints out the instruction to use the class.\n",
    "\n",
    "In step 6, we invoke the chain with another requirement and print the generated code. The generation in this case is acceptable too. The model is smart enough to even generate the necessary import statement for the *hashlib* library. \n",
    "\n",
    "So far, we have used the `codellama model` generate the code and the generation was reasonably good. However, we caution the reader to not just trust the generated output as is. Utmost care should be taken to validate the functionality of the generated code via a suite of unit,integration and functional tests for production scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446afb6-b84a-4c1c-8f24-2efe7b53ef20",
   "metadata": {},
   "source": [
    "# There's more\n",
    "\n",
    "So far, we have used locally hosted models for generation. Let's see how the `ChatGPT` model from Open AI fairs in this regard. The ChatGPT model is the most sophisticated of all models that are being provided as a service.\n",
    "\n",
    "We only need to change what we do in step 3. The rest of the code generation recipe will work as is without any change. The change for step 3 is a simple 3-step process.\n",
    "\n",
    "a. Add the necessary import statement to your list of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32da7ce8-315a-4a29-88b2-513dbeb6e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a6641-eb8b-48bf-a9d2-53893b6fcac8",
   "metadata": {},
   "source": [
    "b. Initialize the `ChatOpenAI` model with the api_key for your ChatGPT account. Though, ChatGPT is free to use via the browser, tha API usage requires a key and requires account credits to make calls to it. Please refer to the documentation at https://openai.com/blog/openai-api for more information. You can store the api_key in a environment variable and read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd077dc-5acb-434f-a5aa-3cbe5659bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "model = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874e300-6554-4864-bf51-b683f4a2826a",
   "metadata": {},
   "source": [
    "c. Invoke the chain. As we observe that the code generated by ChatGPT is more reader friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "814705cc-3d2f-4c58-8481-a31374de6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an example of how you can generate a 512-bit AES hash using the `hashlib` module in Python:\n",
      "\n",
      "```python\n",
      "import hashlib\n",
      "\n",
      "# Create a new AES hash object\n",
      "aes_hash = hashlib.new('sha512')\n",
      "\n",
      "# Update the hash object with your data\n",
      "data = b'Hello, World!'\n",
      "aes_hash.update(data)\n",
      "\n",
      "# Get the hex representation of the 512-bit hash\n",
      "hash_hex = aes_hash.hexdigest()\n",
      "\n",
      "print(\"512-bit AES hash:\", hash_hex)\n",
      "```\n",
      "\n",
      "This code uses the SHA-512 algorithm, which generates a 512-bit hash. You can update the `data` variable with the input you want to hash.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"write a program to generate a 512-bit AES hash\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e5cde-224f-44c5-af92-5b055a8b6434",
   "metadata": {},
   "source": [
    "# <h1><center>Generate SQL query using human defined requirements</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124657c-3706-4eaa-b950-f9f93ee196d2",
   "metadata": {},
   "source": [
    "In this recipe, we will learn how to use an LLM to infer the schema of a database and generate SQL queries based on human input. The human input would be a simple question. The LLM will use the schema information along with the human question to generate the correct SQL query. Also, we will connect to a database that has populated data, execute the generated SQL query and present the answer in a human readable format. Let's get started.\n",
    "\n",
    "For this recipe we will use a SQLite3 DB. Please follow the instructions on https://github.com/jpwhite3/northwind-SQLite3 to setup the DB locally.\n",
    "\n",
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cab5cdf-b2de-427c-9b42-10140c28cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b04b3-47b3-4b73-a780-c3bffcb98cda",
   "metadata": {},
   "source": [
    "2. We define the prompt template and create a `ChatPromptTemplate` instance using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9901a61-5692-4160-82ec-500572a986c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a SQL expert. Based on the table schema below, write just the SQL query without the results that would answer the user's question.:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1915d-7f91-4846-843f-9ae0322a484e",
   "metadata": {},
   "source": [
    "3. We connect to the local DB running on your machine and get the database handle. In this recipe, we are using a file based DB that resides locally on the filesystem. Once you have setup the DB as per the instructions, please set this path to the respective file on your filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be4ec23c-1d0e-475a-a58a-29a35cc93c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///db/northwind-SQLite3/dist/northwind.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5c070-5fcf-4af4-9e90-65689290de1a",
   "metadata": {},
   "source": [
    "4. We define a method that gets will get the schema information using the database handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48049322-fdad-4ea9-a15f-0b88c2a6497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0da4e-1b44-43a3-899f-a859ff6148d9",
   "metadata": {},
   "source": [
    "5. We define a method that will run a query using the database handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "985e1cca-0b6a-418d-9f3d-e998cd7abce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ceb82-0e6e-4f4f-aedf-4744fa9d501a",
   "metadata": {},
   "source": [
    "6. We read the OpenAI API key from the environment variable and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ce006-91e4-4663-a340-fda12c3175f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "model = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736f1de-7e43-44a1-aa5d-2cf548f4b22b",
   "metadata": {},
   "source": [
    "7. We create a chain that uses the database schema, the prompt, the model and an output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0c637bb-5401-4cab-8afd-6f3395bfecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c53f5-e915-4b6a-83b6-21f95073c080",
   "metadata": {},
   "source": [
    "8. We invoke the chain by passing it a simple question. We expect the chain to return a SQL query as part of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab0859e7-9246-4ffa-89f4-50f5d241fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(*) FROM Employees'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_response.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bbadb-cfd7-4e81-943f-4931faea7beb",
   "metadata": {},
   "source": [
    "9. We invoke another query for further testing the LLM whether it can infer the whole schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023444f0-7586-44da-9486-1af972ab8da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT COUNT(*) \\nFROM Employees \\nWHERE HireDate <= DATE('now', '-5 years')\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_response.invoke({\"question\": \"How many employees have been tenured for more than 11 years?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9268b-60d6-423d-a44f-3506003e9ed8",
   "metadata": {},
   "source": [
    "10. We now initialize another template that will instruct the model to use the SQL query and execute it on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def7e4d1-a584-4618-b71a-58ce84350c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5a827-8481-4b08-b9e3-33ad3d991229",
   "metadata": {},
   "source": [
    "11. We create a full chain that uses the previous chain to generate the SQL query, and executes that on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ebd944b-58e7-4d48-b4b8-9d0f5cfd3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_response).assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x: run_query(x[\"query\"]),\n",
    "    )\n",
    "    | prompt_response\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b6441-391a-42c1-8ea7-50801aab0b96",
   "metadata": {},
   "source": [
    "12. We invoke the full chain with the same human question that we asked it earlier. The chain produces a simple human readable answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a0c6807-de45-452b-a2b5-21211da630d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='There are 9 employees in the database.'\n"
     ]
    }
   ],
   "source": [
    "result = full_chain.invoke({\"question\": \"How many employees are there?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2875aa-f328-4647-94da-1c961224feec",
   "metadata": {},
   "source": [
    "13. We invoke the chain with a slighly more complex query. We observe that the simple response from the invocation is indeed accurate. The accompanying screenshot does confirm the accuracy of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d843a8-4e1b-4320-a6f5-98620f64c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The employees who have been tenured for more than 11 years are Nancy Davolio, Andrew Fuller, and Janet Leverling.'\n"
     ]
    }
   ],
   "source": [
    "result = full_chain.invoke({\"question\": \"Give me the name of employees who have been tenured for more than 11 years?\"})\n",
    "print(result)"
   ]
  },
  {
   "attachments": {
    "f8b07299-f2e0-43ed-bf92-a1fdfca0c688.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "244ad457-aab1-45a5-9912-71f02b03307b",
   "metadata": {},
   "source": [
    "Query results that were returned while querying the database manually using the SQLite command line interface.\n",
    "\n",
    "![image.png](attachment:f8b07299-f2e0-43ed-bf92-a1fdfca0c688.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c2195-82c6-45cc-ab8e-45b3a269bc2a",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we define a template. This template defines the instruction or the system prompt that is sent to the model as the task description. In this case, the template defines an instruction to generate a SQL statement based on user's requirement. We use this template to initialize a prompt object. The initialized object is of the type `ChatPromptTemplate`. This object lets us send requirements to the model in an interactive way. We can converse with the model based on our instructions to generate several SQL statements without having to load the model each time.\n",
    "\n",
    "In step 3, we get a handle to the database. We will use this handle in the subsequent calls to make connection with the DB and perform operations on it.\n",
    "\n",
    "In step 4, we define a method named *get_schema* that returns the schema information for all the DB objects like tables, indexes. This schema information is used by the LLM in the following calls to infer the table structure and generate queries from it.\n",
    "\n",
    "In step 5, we define a method named *run_query* that runs the query on the DB and returns the results. The results are used by the LLM in the following calls to infer the result from and generate a human readable, friendly answer.\n",
    "\n",
    "In step 6, we read the *api_key* from an environment variable and initialize the *ChatGPT* model. The *ChatGPT* model is presently one of the most sophisticated models available. Our experiments with using *CodeLlama* for this recipe was returning queries with a lot of noisy characters, as opposed to *ChatGPT*, which was precise and devoid of any noise.\n",
    "\n",
    "In step 7, we create a chain that wires the schema, prompt, model and an output parser. The schema is sourced from the *get_schema* method and passed as a dictionary value to the downstream chain components. The prompt uses the schema to fill in the place holder *schema* element in its template. The model receives the prompt and the schema information and generates the query. The *bind* method of the model is *Runnable* sequence. This method cuts off the output from the model at the first instance of the strings that are passed as the parameter to it. Once the model generates the output, it is parsed by the string parser.\n",
    "\n",
    "In step 8, we invoke the chain and print the results. The generated query by the LLM is accurate and it was able to infer the schema and generate the correct query for our requirements.\n",
    "\n",
    "In step 9, we test the chain further by passing it a slightly more complex sure. On observing the results, we can see it can infer our question based on *tenure* and map it to *HireDate* column as part of the schema. This is a very smart inference that was done automatically by `ChatGPT`.\n",
    "\n",
    "In step 10, we define another prompt template that will let us do generate the query and execute it. It will use the chain that we have created so far, and add the execution components in another chain and invoke that chain. Howeve, at this step, we just generate the template and the prompt instance out of it. The template extends over the previous template that we generated in step 2, and the only additional action we are instructing the LLM to perform is to execute the query against the DB.\n",
    "\n",
    "In step 11, we define another chain. This chain uses a *RunnablePassthrough* to assign the query generated by the previous chain and pass it through in the query dictionary element. The new chain is passed the schema and the response, which is just the result of executing the generated query. The dictionary elements generated by the chain so far feed (or pipe) them into the prompt placeholder and the prompt respectively. The model uses the prompt to emit out a results that is simple and human readable.\n",
    "\n",
    "In step 12, we invoke the chain and print the results. We observe that the results are accurate as per the information verified in the DB. The answer generated is simple and human readable.\n",
    "\n",
    "In Step 13, we invoke the chain again and print the results. The LLM is smart enough to generate and execute the query, infer our answer requirements and map their appropriately with the DB schema and return the results. This is indeed quite impressive. We added a reference screenshot of the data in the DB for our readers to show the accuracy of the results.\n",
    "\n",
    "Though the results generated by the LLMs in this recipe are fairly impressive and accurate, we advise the reader to thoroughly verify their queries and results before taking a system to production. Also important is to ensure that no arbitrary SQL queries can be injected via the users by validating the input. It's best to keep a system answering questions to operate in the context of an account with Readonly permissions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75bb2f1-c1c4-4afe-955f-d8895228b3a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
