{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Natural Language Understanding \n",
    "\n",
    "In this chapter we will explore recipies that will allow us to interpret and understand the text contained in short as well as long passages. Natural Language Understanding (NLU) is a very broad term and and the various systems developed as part of NLU do not interpret or understand a passage of text the same way a human reader would. However, based on the specificity of the task, we can create some applications that can be combined together to generate an interpretation or understanding that can be used to solve a given problem related to text processing. As part of this chapter, we will build recipes for the following tasks.\n",
    "\n",
    "* Answer questions from a short text passage\n",
    "* Answer questions from a long text passage\n",
    "* Answer questions from a document corpus in an extractive manner\n",
    "* Answer questions from a document corpus in an abstractive manner\n",
    "* Summarize text using pre-trained models based on Transformers\n",
    "* Sentence Entailment detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Question Answering</center></h1>\n",
    "\n",
    "To get started with question answering, we will start with a simple recipe which can answer a question from a short passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Ready\n",
    "\n",
    "As part of this chapter, we will use the libraries from the HuggingFace site (huggingface.co). For this recipe, we will use the BertForQuestionAnswering and BertTokenizer modules from the transformers package. The `BertForQuestionAnswering` model uses the base BERT large uncased model that was trained on the SQuAD dataset and fine-tuned for the question answering task. This pre-trained model can be used to load a text passage and answer questions based on the contents of the passage.\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "`pip install datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it\n",
    "\n",
    "In this recipe, we will load a pretrained model that has been trained on the SQuAD dataset (https://huggingface.co/datasets/squad). We will initialize a context passage and use the modeal to answer a couple of questions based on the passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', device_map=device)\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Initialize a text passage. This passage will be used as the context to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`context = \"The cat had no business entering the neighbors garage, but she was there to help. The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. She said it seemed like it was trying to get into her home, and that she was afraid for her life. The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground.\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Initialize a question with relevant text and use it to generate an answer via the pipeline and the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where was the cat trying to enter?\"\n",
    "result = question_answer_pipeline(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{'score': 0.25550469756126404, 'start': 33, 'end': 54, 'answer': 'the neighbors garage,'}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Print the exact text answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`the neighbors garage,`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Ask another question for the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What did the cat do after entering the garage\"\n",
    "result = question_answer_pipeline(question=question, context=context)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hit her in the face, knocking her to the ground.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "The program does the following things.\n",
    "\n",
    "1. It initializes a `question-answering` pipeline based on the pre-trained `BertForQuestionAnswering` model and `BertTokenizer` tokenizer.\n",
    "\n",
    "2. It further initializes a context passage and a question and emits the output of the answer based on these two parameters. It also prints the exact text of the answer.\n",
    "\n",
    "3. It asks a follow-up question to the same pipeline by just changing the question text and prints the exact text answer as to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 1, we do the necessary imports of the required modules and packages.\n",
    "\n",
    "In step 2, we initialize the model and tokenizer respectively using the pre-trained `bert-large-uncased-whole-word-masking-finetuned-squad` artifacts. These will be downloaded from the HuggingFace site if they are not present locally on the machine as part of these calls. We have chosen the specific model and tokenizer for our recipe, but feel free to explore the other models on the HuggingFace site that might suit your needs. As a generic step for this as well as the following recipe, we discover whether there are any GPU devices in the system and attempt to use it. If a GPU is not detected, we use the CPU instead.\n",
    "\n",
    "In step 3, we initialize a `question-answering` pipeline with the model and tokenizer. The task type for this pipeline is set to `question-answering`.\n",
    "\n",
    "In step 4, we initialize a context passage. This passage was generated as part of our `Text Generation via Transformers` example in the chapter on Transformers previously.\n",
    "\n",
    "In step 5, we initialize a question text and invoke the pipeline with the context and question and store the result in a variable. The type of the result is a python `dict` object.\n",
    "\n",
    "In step 6, we print the value of the result. The `score` value shows the probability of the answer. The `start` and `end` values denote the start and end character indices in the context passage that constitute the answer. The `answer` value denotes the actual text of the answer.\n",
    "\n",
    "In step 7, we print the exact text of the answer.\n",
    "\n",
    "In step 8, we initialize a different question text and print its exact text answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', device_map=device)\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', device=device)\n",
    "\n",
    "question_answer_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)\n",
    "\n",
    "context = \"The cat had no business entering the neighbors garage, but she was there to help. The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. She said it seemed like it was trying to get into her home, and that she was afraid for her life. The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground.\"\n",
    "\n",
    "question = \"Where was the cat trying to enter?\"\n",
    "result = question_answer_pipeline(question=question, context=context)\n",
    "\n",
    "print(result)\n",
    "\n",
    "print(result['answer'])\n",
    "\n",
    "question = \"What did the cat do after entering the garage\"\n",
    "result = question_answer_pipeline(question=question, context=context)\n",
    "print(result['answer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Answer questions from a long text passage</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous recipe we learnt an approach to extract answer for a question, given a context. This pattern involves the model to retrieve the answer from the given context. The model cannot answer a question that is not contained in the context. While this does serve a purpose where we want an answer from a given context. This type of question-answering system is defined as `Closed Domain Question Answering (CDQA)`.\n",
    "\n",
    "There is another system of question answering that can answer questions that are general in nature. These systems are trained on larger corpora. This training provides them the ability to answer questions that are open in nature. These systems are called `Open Domain Question Answering` systems or ODQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install en_odqa_infer_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "odqa = build_model('en_odqa_infer_wiki', download=True)\n",
    "result = odqa(['What is the name of the Doctor in Star Trek?'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install kbqa_cq_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Ready\n",
    "\n",
    "As part of this recipe, we will use the **Deep Pavlov** (https://deeppavlov.ai) based ODQA system to answer an open question. We will use the `deeppavlov` library along with the Knowledge Base Question Answering (KBQA) model. This model has been trained on Engligh Wikidata as a knowledge base. It uses various NLP techniques like entity linking and disambiguation, knowledge graphs etc to extract the exact answer to the question. \n",
    "\n",
    "*Install the deeppavlov library*\n",
    "\n",
    "`pip install deeppavlov` \n",
    "\n",
    "*Install the model*\n",
    "\n",
    "`python -m deeppavlov install kbqa_cq_en` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it\n",
    "\n",
    "In this recipe, we will initialize the KBQA model based on the DeepPavlov library and use it to answer an open question. The steps for the recipe are as follows.\n",
    "\n",
    "1. Do the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the KBQA model. The identifier for the model is *kbqa_cq_en* and that is passed to the *build_model* method as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbqa_model = build_model('kbqa_cq_en', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We use the initialized model and pass it a couple of questions that we want to be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kbqa_model(['What is the capital of Egypt?', 'Who is Bill Clinton\\'s wife?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We print the result as returned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[['Cairo', 'Hillary Clinton'], [['Q85'], ['Q6294']], [['SELECT ?answer WHERE { wd:Q79 wdt:P36 ?answer. }'], ['SELECT ?answer WHERE { wd:Q1124 wdt:P26 ?answer. }']]]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "The program does the following things:\n",
    "\n",
    "In step 1, we import the necessary module from the deeppavlov library.\n",
    "\n",
    "In step 2, we initialize the KBQA model using the `build_model` method. We set the *download* argument to *True* so that the model is downloaded as well in case its missing locally.\n",
    "\n",
    "In step 3, we invoke the QA model and pass two open-ended questions to it in a python string array.\n",
    "\n",
    "In step 4, we print the result returned by the model. The result contains three arrays.\n",
    "    \n",
    "    - The first array contains the exact answers to the question ordered in the same way as the original input. In this case the answers \"Cairo\" and \"Hillary Clinton\" are in the same order as the questions they pertain to.\n",
    "    - The other two arrays contain the internal artifacts that is used by the model to generate the answer. For more information on the internal details of the working of DeepPavlov, we recommend the reader to refer the package reference at https://deeppavlov.ai ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "kbqa_model = build_model('kbqa_cq_en', download=False)\n",
    "\n",
    "result = kbqa_model(['What is the capital of Egypt?', 'Who is Bill Clinton\\'s wife?'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Question Answering on a Document Corpus</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the use cases where we have document corpus that contains a large number of documents, its not feasible to load the document content at runtime to answer a question. Such an approach would lead to long query times and would not be suitable for production grade systems. In this recipe we will learn how to pre-process the documents and tranform them in a form for faster reading, indexing, and retrieval that allows the system to extract the answer for a given question with short query times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Ready\n",
    "\n",
    "As part of this recipe, we will use the **Haystack** (https://haystack.deepset.ai/) framework to build a QA system that can answer questions off a document corpus. We will download a dataset based on Game of Thrones and index it. In order for our QA system to be performant, we will need index the documents beforehand. Once the documents are indexed, answering a question follows a two-step process.\n",
    "\n",
    "1. Retriever - Since we have a large number of documents. Scanning each document for fetching as answer is not a feasible approach. We will first retrive a set of candidate documents that can possibly contain an answer to our question. This step is performed using a Retriever component. It searches through the pre-created index to filter the number of documents that we will need to scan to retrieve the exact answer.\n",
    "\n",
    "2. Reader - Once we have a candidate set of documents which can contain the answer, we will search these documents to retrieve the exact answer to our question.\n",
    "\n",
    "We will discuss the details of these components throughout this recipe. To start with, let's setup the pre-requisites.\n",
    "\n",
    "*Install the latest release of haystack.*\n",
    "\n",
    "`pip install farm-haystack` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install farm-haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do it\n",
    "\n",
    "1. Do the necessary imports\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import BM25Retriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.pipelines.standard_pipelines import TextIndexingPipeline\n",
    "from haystack.utils import fetch_archive_from_http, print_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fetch the documents from the source and save it to a local folder. Once the documents are fetched, load the files from the dataset that we need to index. We also print the number of files that we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "doc_dir = \"data/got_dataset\"\n",
    "fetch_archive_from_http(\n",
    "            url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip\",\n",
    "            output_dir=doc_dir,\n",
    "        )\n",
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "print(len(files_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We initialize a document store based on the files. We create an indexing pipeline based on the document store and execute the indexing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "indexing_pipeline = TextIndexingPipeline(document_store)\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Once we have loaded the documents, we initialize our retriever and reader instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now create a pipeline that we can use to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)\n",
    "prediction = pipe.run(\n",
    "            query=\"Who is the father of Arya Stark?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We print the answers to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Query: Who is the father of Arya Stark?'\n",
      "'Answers:'\n",
      "[   <Answer {'answer': 'Eddard', 'type': 'extractive', 'score': 0.993372917175293, 'context': \"s Nymeria after a legendary warrior queen. She travels with her father, Eddard, to King's Landing when he is made Hand of the King. Before she leaves,\", 'offsets_in_document': [{'start': 207, 'end': 213}], 'offsets_in_context': [{'start': 72, 'end': 78}], 'document_ids': ['9e3c863097d66aeed9992e0b6bf1f2f4'], 'meta': {'_split_id': 3}}>,\n",
      "    <Answer {'answer': 'Ned', 'type': 'extractive', 'score': 0.9753613471984863, 'context': \"k in the television series.\\n\\n====Season 1====\\nArya accompanies her father Ned and her sister Sansa to King's Landing. Before their departure, Arya's h\", 'offsets_in_document': [{'start': 630, 'end': 633}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['7d3360fa29130e69ea6b2ba5c5a8f9c8'], 'meta': {'_split_id': 10}}>,\n",
      "    <Answer {'answer': 'Lord Eddard Stark', 'type': 'extractive', 'score': 0.9177322387695312, 'context': 'rk daughters.\\n\\nDuring the Tourney of the Hand to honour her father Lord Eddard Stark, Sansa Stark is enchanted by the knights performing in the event.', 'offsets_in_document': [{'start': 280, 'end': 297}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_ids': ['5dbccad397381605eba063f71dd500a6'], 'meta': {'_split_id': 3}}>,\n",
      "    <Answer {'answer': 'Ned', 'type': 'extractive', 'score': 0.8396496772766113, 'context': \" girl disguised as a boy all along and is surprised to learn she is Arya, Ned Stark's daughter. After the Goldcloaks get help from Ser Amory Lorch and\", 'offsets_in_document': [{'start': 848, 'end': 851}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['257088f56d2faba55e2ef2ebd19502dc'], 'meta': {'_split_id': 31}}>,\n",
      "    <Answer {'answer': 'King Robert', 'type': 'extractive', 'score': 0.6922298073768616, 'context': \"en refuses to yield Gendry, who is actually a bastard son of the late King Robert, to the Lannisters.  The Night's Watch convoy is overrun and massacr\", 'offsets_in_document': [{'start': 579, 'end': 590}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_ids': ['4d51b1876e8a7eac8132b97e2af04401'], 'meta': {'_split_id': 4}}>]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"all\")  ## Choose from `minimum`, `medium`, and `all`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we specify a folder that will be used to save our dataset. We retrieve the dataset from the source. The second parameter to the \n",
    "`fetch_archive_from_http` method is the folder where the dataset will be downloaded to. We set the parameter to the folder which we defined in the previous step. The `fetch_archive_from_http` decompresses the archive `.zip` file and extracts all of them in the same folder. We read from the folder and create a list of files contained in the folder. \n",
    "\n",
    "In step 3, we initialize an `InMemoryDocumentStore` instance. In this method call, we set the argument `use_bm25` as `True`. The document store uses *bm25* as the algorithm for the retriever step. The *bm25* (Best Match 25) algorithm is simple bag-of-words based algorithm that uses a scoring function that utilizes the number of instances a term is present in the document and also on the length of the document. Note that there are various other DocumentStore options like `ElasticSearch`, `OpenSearch` etc.  We used an `InMemoryDocumentStore` document store to keep the recipe simple and focus on the retriever and reader concepts. For a QA system to work in a high-performance production system, it is recommended to use a different document store than an in-memory one. We recommend the reader to refer to https://docs.haystack.deepset.ai/docs/document_store and use an appropriate document store based on their production grade requirements. Once the document store is initialized, we also create an indexing pipeline to index the files into the initialized document store. Indexing the files allows us to search through the content faster.\n",
    "\n",
    "In step 4, we initialize the retriever and the reader components. The `BM25Retriever` uses the *bm25* scoring function to retrieve the initial set of documents. For the reader we initialize the `FARMReader` object. It is based on deepsets FARM framework that can utilize the QA models from HuggingFace. In our case we use the `deepset/roberta-base-squad2` model to be used as a reader. The `use_gpu` argument can be set appropriately based on whether your device has a GPU.\n",
    "\n",
    "In step 5, after having initialized the retriever and reader in the previous step, we want to combine them for querying. The `pipeline` abstraction from Haystack framework allows us to integrate the reader and retriever together using a series of pipelines that address different use cases. In this instance we will use the `ExtractiveQAPipeline` for our QA system. After initialization of the pipeline, we generate the answer to a question from the \"Game of Thrones\" series. The `run` method takes the question as the query. The second argument `params` dictates how the results from the retriever and reader are combined to present the answer.\n",
    "\n",
    "- \"Retriever\": {\"top_k\": 10} - The `top_k` keyword argument specifies that the top-k (in this case, 10) results from the retriever are used by the reader to search for the exact answer.\n",
    "- \"Reader\": {\"top_k\": 5} - The `top_k` keyword argument specifies that the top-k (in this case, 5) results from the reader are presented as the output of the method.\n",
    "\n",
    "In step 6, we print the answers as processed and returned by the pipeline. The system prints out the exact answers along with the associated context that it used to extract the answer from. Note that we use the value of *minimum* for the `details` argument. This argument only presents the answer and the context. Setting the value of *mediium* for the `details` argument provides the relative score of each-answer. This score can be used to filter out the results further based on the accuracy requirements of the system. Using the *all* value for the same argument prints out start and end spans for the answer along with all the auxilliary information. We encourage the reader to make a suitable choice on the basis of their requirement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import BM25Retriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.pipelines.standard_pipelines import TextIndexingPipeline\n",
    "from haystack.utils import fetch_archive_from_http, print_answers\n",
    "\n",
    "doc_dir = \"data/got_dataset\"\n",
    "fetch_archive_from_http(\n",
    "            url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip\",\n",
    "            output_dir=doc_dir,\n",
    "        )\n",
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "print(len(files_to_index))\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "indexing_pipeline = TextIndexingPipeline(document_store)\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)\n",
    "prediction = pipe.run(\n",
    "    query=\"Who is the father of Arya Stark?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")\n",
    "\n",
    "print_answers(prediction, details=\"minimum\")  ## Choose from `minimum`, `medium`, and `all`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Abstractive Question Answering on a Document Corpus</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous recipe, we learnt how to build a QA system based on the document corpora. The answers that were retrieved were extractive in nature, i.e. the answer snippet was a piece of text copied verbatim from the document source. \n",
    "\n",
    "In this recipe, we will build a QA system that will provide answers that are abstractive in nature. An abstractive answer is more readable by end users compared to an extractive one. We will load the `bilgeyucel/seven-wonders` dataset from the HuggingFace site and initialize a retriever from it. This dataset has content about the seven wonders of the ancient world. For generating the answers, we will use the PromptNode component from the Haystack framework to setup a pipeline that can generate answers in an abstractive fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate, AnswerParser\n",
    "from haystack.pipelines import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As part of this step, we load the `bilgeyucel/seven-wonders` dataset. Once the dataset is downloaded, we initialize an in-memory document store based on the BM25 algorithm, as we did in the previous recipe. We write the documents in the datset to the document store. Once the document store is initilized, we create a retriever component based on the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(dataset)\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. As part of this step, we initialize a prompt template. We can define the task we want the model to perform as a simple instruction in English. We use this *PromptTemplate* instance to create a *PromptNode* component instance. This instance is initialized with the *google/flan-t5-large* model and the *PromptTemplate* we created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=rag_prompt, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We now create a pipeline and add the `retriever` and `prompt_node` components that we initialized in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Once the pipeline is set up, we use it to answer questions on the content based on the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe.run(query=\"What is the Great Pyramid of Giza?\")\n",
    "print(output[\"answers\"][0].answer)\n",
    "output = pipe.run(query=\"Where are the hanging gardens?\")\n",
    "print(output[\"answers\"][0].answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Great Pyramid of Giza was built in the early 26th century BC during a period of around 27 years.[3]\n",
    "\n",
    "The Hanging Gardens of Semiramis are the only one of the Seven Wonders for which the location has not been definitively established."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we load the `bilgeyucel/seven-wonders` dataset into an in-memory document store. This dataset has been created out of the Wikipedia pages of Seven Wonders of the Ancient World (https://en.wikipedia.org/wiki/Wonders_of_the_World). This dataset has been pre-processed and uploaded to the HuggingFace space. This dataset can be easily downloaded by using the datasets module from HuggingFace. We use the `InMemoryDocumentStore` as our document store, with BM25 as the choice of search algorithm. We write the documents from the dataset into the document store. To have a performant query time performance `write_documents` automatically optimizes how the documents are written. Once the documents are written into, we initialize the retriver based on BM25, similar to our previous recipe.\n",
    "\n",
    "In step 3, we don't initialize a reader unlike the previous recipe. Instead, we initialize a PromptTemplate that allows us to define the way the answers will be generated. The `prompt` argument can be used to define the task that we want to perform. It also takes two internal arguments *document* and *query*. These variables are expected to be in the execution context at runtime. In our example, we join all the documents together to form one string that can be used for the search. We also define that the query will be supplied as the *query* keyword. Please refer to the prompt engineering guide on Haystack on how to generate prompts for your use cases (https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines). \n",
    "The second argument `output_parser` takes an *AnswerParser* object. This object instructs the PromptNode object to store the results in the `answers` element. \n",
    "After defining the prompt, we initialize a PromptNode object with a model and the prompt template. We use the `google/flan-t5-large` model as the answer generator. This model is based on the Google T5 language model and has been fine-tuned (*flan* stands for `Finetuning language models`). One of the fine tuning steps as part of this model training was to operate on human written instructions as tasks. This allowed the model to perform different downstream tasks on instructions alone and reduce the need for any few-shot examples to be trained on.\n",
    "\n",
    "In step 4, we initilize a pipeline and its retriever and prompt node components. The retriever component operates on the query supplied by the user and generates a set of results. These results are passed to the prompt node and it uses the configured `flan-t5-model` to generate the answer.\n",
    "\n",
    "In step 5, we perform a query to the QA system with two abstract questions about the wonders and it returns the answers that it extracted from the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate, AnswerParser\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(dataset)\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Generate an answer for the given question in less than 50 words.\n",
    "                \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=rag_prompt, use_gpu=True)\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])\n",
    "\n",
    "\n",
    "output = pipe.run(query=\"When was the Great Pyramid of Giza built?\")\n",
    "print(output[\"answers\"][0].answer)\n",
    "\n",
    "output = pipe.run(query=\"Where are the hanging gardens?\")\n",
    "print(output[\"answers\"][0].answer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Text Summarization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore techniques for performing text summarization. Generating a summary for a long passage of text allows NLP practitioners to extract the relevant information for their use cases and use these summaries for other downstream tasks. As part of summarization, we will explore recipes that use Transformer models for generating the summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Text Summarization with Google T5 model</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first recipe for summarization will use the Google T5 (Text-to-Text Transfer Transformer) model for summarization. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As part of this step. we initialize a passage of text. We also initialize a pipeline instance. This pipeline has the task defined as `summarization` and the model used in the pipeline is the Google `t5-large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is by no means easy to see. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.\"\n",
    "pipeline_instance = pipeline(\"summarization\", model=\"t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We now use the pipeline_instance initialized in the previous step and pass the text passage to it to perform the summarization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = pipeline_instance(passage, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Once the summarization step is complete, we extract the result out of the output and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline_result[0][\"summary_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`the color of animals is by no means a matter of chance; it depends on many considerations . in the majority of cases, coloring tends to protect the animal from danger . there are, however, not a few cases in which vivid colors are themselves protective .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we initialize the input passage that we need to summarize alond with the pipeline. Since we defined the task as `summarization`, the object returned by the pipeline module is of type `SummarizationPipeline`. We also passed `t5-large` as the model parameter for the pipeline. This model is based on the Encoder-Decoder Transformer model, and acts as a pure sequence-to-sequence model. That means the input and output to/from the model are text sequences. This model was pre-trained using the denoising objective of finding masked words in a sentence followed by fine-tuning on specific downstream tasks like summarization, textual entailment, language translation etc. \n",
    "\n",
    "In step 3, we execute the summarization step using the pipeline. We pass the passage string as the first argument but a string array can be passed as well if multiple sequences are to be summarized. We passed `max_length=512` as the second argument The T5 model is memory intensive and the compute requirements grow quadratically with the increase in the input text length.\n",
    "\n",
    "In Step 4, we extract the summary text from the result emitted by the pipeline. The pipeline returns a list of dictionaries. Each list item corresponds to the input argument. In this case, since we passed only one string as input, the first item in the list is the output dictionary that contains our summary. The summary can be retrieved by indexing the dictionary on the `summary_text` element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "passage = \"The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is by no means easy to see. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.\"\n",
    "pipeline_instance = pipeline(\"summarization\", model=\"t5-large\")\n",
    "\n",
    "pipeline_result = pipeline_instance(passage, max_length=512)\n",
    "\n",
    "result = pipeline_result[0][\"summary_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the color of animals is by no means a matter of chance; it depends on many considerations . in the majority of cases, coloring tends to protect the animal from danger . there are, however, not a few cases in which vivid colors are themselves protective ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how we can generate a summary using the T5 model, we can use the same generic code framework and tweak it slightly to use other models to generate summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below would be common for the other summarization recipes that we are using. We added an extra variable named `device` that we will use in our pipelines. We set this variable to value of the device that we will use to generate the summary. If a `GPU` is present and configured in the system, it will be use, else the summarization will be performed using the `CPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "passage = \"The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is by no means easy to see. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we use the BART model from Facebook. This model was pre-trained using a noising function on a piece of text, followed by setting the training objective to generate the original text by removing the noise from the text. The model was further fine-tuned using the on the `CNN DailyMail` dataset for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "pipeline_result = pipeline_instance(passage, max_length=512)\n",
    "result = pipeline_result[0][\"summary_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. There are, however, not a few cases in which vivid colors are themselves protective. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe from the generated summary, it is verbose and extractive in nature. Let's try generating a summary with a another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we use the `PEGASUS` model from Google for summarization. This model is a Transformer based encoder-decoder model which was pre-trained with a large news and web page corpus on a training objective of detecting important sentences. This model was further fine-tuned for summarization on a small dataset. This model generates abstract summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = pipeline(\"summarization\", model=\"google/pegasus-large\", device=device)\n",
    "pipeline_result = pipeline_instance([passage, passage], max_length=512)\n",
    "result = pipeline_result[0][\"summary_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe from the generated summary, it is concise and abstractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many new and improved models for summarization are always in the works, we recommend that the reader refer the models on the HuggingFace site (https://huggingface.co/models?pipeline_tag=summarization) and make the respective choice based on their requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Textual Entailment</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore techniques to detect textual entailment, given a set of two sentences. The first sentence in the set is the `premise`, which sets up a context. The second sentence is the `hypothesis`. Textual Entailment identifies the contextual relationship between the `premise` and the `hypothesis`. These relationships can be be of 3 types defined as:\n",
    "\n",
    "* Entailment - The hypothesis supports the premise.\n",
    "* Contradiction - The hypothesis contradicts the premise.\n",
    "* Neutral - The hypothesis does not support or contradict the premise.\n",
    "\n",
    "In this recipe, we will initialize different sets of sentences that are related in each of the above defined relationships and explore methods to detect these relationships. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the device, the tokenizer and the model. In this case, we are using the Google `t5-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")       \n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False, device=device)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We initialize the premise and the hypothesis sentences. In this case, the hypothesis supports the premise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"The corner coffee shop serves the most awesome coffee I have ever had.\"\n",
    "hypothesis = \"I love the coffee served by the corner coffee shop.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In this step we tokenize the premise and the hypothesis sentences and have the model generate the output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"mnli premise: \" + premise + \" hypothesis: \" + hypothesis, return_tensors=\"pt\").input_ids\n",
    "entailment_ids = model.generate(input_ids.to(device), max_new_tokens=20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In this step we decode the generated prediction tokens from the model and print the result. In this case, the generated prediction by the model is `entailment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = tokenizer.decode(entailment_ids[0], skip_special_tokens=True, device=device)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works\n",
    "\n",
    "In step 1, we do the necessary imports.\n",
    "\n",
    "In step 2, we initialize the tokenizer with the `t5-small` model. We set the legacy flag to *False* since we don't need to use the legacy behavior of the model. We set the device value based on whatever device we have available in our execution environment. Similarly for the model, we set the model name and device parameter similar to the Tokenizer. We set the parameter *return_dict* as True so that we get the model results as a dictionary instead of tuple.\n",
    "\n",
    "In step 3, we initialize the premise and the hypothesis sentences.\n",
    "\n",
    "In step 4, we call the tokenizer with the `mnli premise` and `hypothesis` values. This is a simple text concatenation step to set up the tokenizer for the `entailment` task. We read the `input_ids` property to get the token identifiers for the concatenated string. Once we have the token IDs, use the model to generate the entailment prediction. This returns a list of tensors with the predictions that we use in the next step.\n",
    "\n",
    "In step 5, we call the `decode` method of the tokenizer and pass it the first tensor (or vector) of the tensors that were returned by the `generate` call of the model. We also instruct the tokenizer to skip the special tokens that are used by the tokenizer internally. The tokenizer generates the string label from the vector that is passed in. We print the prediction result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There's more\n",
    "\n",
    "Now that we have shown an example with the case of entailment with a single sentence, the same framework can be used to process a batch of sentences to generate entailment predictions. We will tailor steps 3,4, and 5 from the previous recipe for this example. We initialize an array of 2 sentences for both premise and hypothesis respectively. Both the premise sentences are the same, while the hypotheses sentences are of `entailment` and `contradiction` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = [\"The corner coffee shop serves the most awesome coffee I have ever had.\", \"The corner coffee shop serves the most awesome coffee I have ever had.\"]\n",
    "hypothesis = [\"I love the coffee served by the corner coffee shop.\", \"I find the coffee served by the corner coffee shop too bitter for my taste.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an array of sentences for both premises and hypotheses, we create an array of concatenated inputs that combine the tokenizer instruction. This array is used to pass to the tokenizer and we use the token IDs returned by tokenizer in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premises_and_hypotheses = [f\"mnli premise: {pre} hypothesis: {hyp}\" for pre, hyp in zip(premise, hypothesis)]\n",
    "input_ids = tokenizer(text=premises_and_hypotheses, padding=True,\n",
    "                      return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the predictions using the same methodology that we used earlier. However, in this step, we generate the inference label by iterating through the tensors returned by the models output and printing the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_ids = model.generate(input_ids.to(device), max_new_tokens=20)\n",
    "for _tensor in entailment_ids:\n",
    "    entailment = tokenizer.decode(_tensor, skip_special_tokens=True, device=device)\n",
    "    print(entailment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Below is the whole code as a single script that can be validated by a development editor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False, device=device)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True, device_map=device)\n",
    "premise = \"The corner coffee shop serves the most awesome coffee I have ever had.\"\n",
    "hypothesis = \"I love the coffee served by the corner coffee shop.\"\n",
    "# hypothesis = (\"I love the waffle served by the corner coffee shop.\")\n",
    "hypothesis = (\"I find the coffee served by the corner coffee shop too bitter for my taste.\")\n",
    "input_ids = tokenizer(\"mnli premise: \" + premise + \" hypothesis: \" + hypothesis,\n",
    "                      return_tensors=\"pt\").input_ids\n",
    "entailment_ids = model.generate(input_ids.to(device), max_new_tokens=20)\n",
    "entailment = tokenizer.decode(entailment_ids[0], skip_special_tokens=True, device=device)\n",
    "print(entailment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
