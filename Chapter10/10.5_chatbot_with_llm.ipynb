{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19169349-1712-456a-983a-1ab4edfb6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "import bs4\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13133579-00df-4800-b1ba-214ba5d99e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings_provider = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3846165-28a8-4710-9d50-046cf754743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "     ]\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af21b10b-bb46-4674-ba5d-d431d89843f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b444643b-3840-480f-bf78-230b26af9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = DocArrayInMemorySearch.from_documents(document_chunks, embeddings_provider)\n",
    "retriever = vector_store.as_retriever()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b782f90c-5805-45c0-8ab1-dd56811f7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592c6bc6-d43d-4dff-9e2f-c33126aa5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_chain = contextualize_q_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c239416c-ade7-480b-be49-96a327ca32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28315df-1999-46d0-8d04-f8c90ac8e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a703c1-1830-4e8c-b018-9bdb383fd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=contextualized_question | retriever | format_docs\n",
    "        )\n",
    "        | qa_prompt\n",
    "        | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7508be31-de6c-49e4-9d3a-29ba99ad8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. LLMs have become increasingly popular in recent years due to their ability to process and generate text at scale, and have a wide range of applications such as language translation, text summarization, and content generation.\n",
      "\n",
      "There are several types of LLMs, including:\n",
      "\n",
      "1. Neural Network-based Models: These models use neural networks to learn the patterns and structures of language from large datasets of text. Examples include Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers.\n",
      "2. Statistical Models: These models use statistical techniques such as probabilistic modeling and maximum likelihood estimation to learn the patterns and structures of language from large datasets of text. Examples include N-gram models and Markov Chain models.\n",
      "3. Hybrid Models: These models combine different techniques such as neural networks and statistical models to learn the patterns and structures of language. Examples include hybrid RNNs and hybrid LSTMs.\n",
      "4. Generative Adversarial Networks (GANs): These models use two neural networks, a generator and a discriminator, to generate new text that is similar to a given dataset. Examples include WaveNet and the Transformer.\n",
      "\n",
      "The key advantages of LLMs are:\n",
      "\n",
      "1. Improved Accuracy: LLMs can generate more accurate and coherent language outputs than traditional rule-based models.\n",
      "2. Increased Efficiency: LLMs can process and generate text at scale, making them more efficient than traditional models.\n",
      "3. Flexibility and Adaptability: LLMs can be trained on a wide range of datasets and can adapt to different language styles and genres.\n",
      "4. Improved Consistency: LLMs can generate more consistent and coherent language outputs than traditional models, especially in cases where the input is noisy or ambiguous.\n",
      "\n",
      "The main challenges and limitations of LLMs are:\n",
      "\n",
      "1. Training Time: Training LLMs can be computationally expensive and time-consuming, especially for large datasets.\n",
      "2. Overfitting: LLMs can overfit the training data, resulting in poor generalization performance on unseen data.\n",
      "3. Limited Domain Knowledge: LLMs may not capture domain-specific knowledge or nuances, leading to suboptimal performance in certain contexts.\n",
      "4. Ethical Concerns: LLMs raise ethical concerns related to bias, privacy, and misuse, especially in applications such as language translation and content generation.\n",
      "\n",
      "In summary, LLMs are powerful tools for processing and generating text at scale, offering improved accuracy, efficiency, flexibility, and consistency compared to traditional rule-based models. However, they also have limitations and challenges related to training time, overfitting, limited domain knowledge, and ethical concerns.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is a large language model?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "print(ai_msg)\n",
    "chat_history.extend([HumanMessage(content=question), AIMessage(content=ai_msg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c668bb32-9830-49e8-9313-46e581ff71c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Certainly! When we refer to a language model as \"large,\" we are essentially highlighting its scale and capabilities. Here are some key reasons why LLMs are called large:\n",
      "\n",
      "1. Size of the Model: The most obvious reason is the sheer size of the model itself. LLMs typically involve complex neural network architectures with numerous layers, nodes, or parameters. These models require significant computational resources to train and run, making them \"large\" in terms of their complexity and scope.\n",
      "2. Scale of Training Data: Another important aspect is the scale of the training data used to train LLMs. Modern language models are typically trained on vast amounts of text data, which can range from tens of thousands to millions or even billions of words. The larger the dataset, the more accurate and robust the model becomes.\n",
      "3. Generative Capabilities: Large language models are designed to generate coherent and natural-sounding text. They have learned patterns and structures of language from large datasets, which allows them to create novel text that is similar but not identical to the training data. This generative capability is what sets LLMs apart from smaller, more rule-based models.\n",
      "4. Adaptability and Flexibility: Large language models can adapt to different genres, styles, and domains of text. They are capable of generating text that is contextually appropriate, coherent within a given context, or even creative in their responses. This flexibility is achieved through sophisticated architectures, training methods, and the sheer scale of the models.\n",
      "5. Ability to Handle Complexity: Large language models are capable of handling complex linguistic structures, such as multi-step reasoning, logical arguments, or even humor. They can integrate various types of knowledge and generate text that is contextually appropriate and semantically meaningful.\n",
      "6. Potential for Improved Generalization: Larger models tend to generalize better to unseen data, which means they perform better on tasks beyond the training data. This is particularly important in natural language processing (NLP) applications where the model needs to be able to handle diverse and dynamic linguistic phenomena.\n",
      "7. Robustness Against Overfitting: Large language models have a greater capacity for learning and forgetting, which helps them avoid overfitting to the training data. This means they can adapt more effectively to unseen data and perform better on real-world tasks.\n",
      "8. Enhanced Performance in Multitask Learning: LLMs are often trained on multiple tasks simultaneously, such as language translation, text summarization, and question answering. The larger the model, the better it performs in these multitask settings due to the sharing of knowledge and representations across tasks.\n",
      "9. Increased Efficiency with Larger Models: While training large models can be computationally expensive, they tend to be more efficient in terms of inference time (i.e., generating text) compared to smaller models. This is because larger models have a greater number of parameters that can be shared across multiple inputs, leading to faster response times and more efficient use of computational resources.\n",
      "10. Future Research Directions: The field of natural language processing is rapidly advancing, with new techniques and architectures being developed continuously. Large language models are likely to play a central role in many of these developments, providing a foundation for more complex and sophisticated NLP tasks.\n",
      "\n",
      "In summary, the term \"large\" when applied to language models refers to their size, complexity, adaptability, and potential for improved generalization and performance in multitask settings. These characteristics make them particularly useful for a wide range of natural language processing applications.\n"
     ]
    }
   ],
   "source": [
    "second_question = \"Can you explain the reasoning behind calling it large?\"\n",
    "second_answer = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n",
    "print(second_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
